<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Machine learning to predict good weight lifting form using sensor data</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>

<!-- Styles for R syntax highlighter -->
<style type="text/css">
   pre .operator,
   pre .paren {
     color: rgb(104, 118, 135)
   }

   pre .literal {
     color: #990073
   }

   pre .number {
     color: #099;
   }

   pre .comment {
     color: #998;
     font-style: italic
   }

   pre .keyword {
     color: #900;
     font-weight: bold
   }

   pre .identifier {
     color: rgb(0, 0, 0);
   }

   pre .string {
     color: #d14;
   }
</style>

<!-- R syntax highlighter -->
<script type="text/javascript">
var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.cpp=function(){var a={keyword:{"false":1,"int":1,"float":1,"while":1,"private":1,"char":1,"catch":1,"export":1,virtual:1,operator:2,sizeof:2,dynamic_cast:2,typedef:2,const_cast:2,"const":1,struct:1,"for":1,static_cast:2,union:1,namespace:1,unsigned:1,"long":1,"throw":1,"volatile":2,"static":1,"protected":1,bool:1,template:1,mutable:1,"if":1,"public":1,friend:2,"do":1,"return":1,"goto":1,auto:1,"void":2,"enum":1,"else":1,"break":1,"new":1,extern:1,using:1,"true":1,"class":1,asm:1,"case":1,typeid:1,"short":1,reinterpret_cast:2,"default":1,"double":1,register:1,explicit:1,signed:1,typename:1,"try":1,"this":1,"switch":1,"continue":1,wchar_t:1,inline:1,"delete":1,alignof:1,char16_t:1,char32_t:1,constexpr:1,decltype:1,noexcept:1,nullptr:1,static_assert:1,thread_local:1,restrict:1,_Bool:1,complex:1},built_in:{std:1,string:1,cin:1,cout:1,cerr:1,clog:1,stringstream:1,istringstream:1,ostringstream:1,auto_ptr:1,deque:1,list:1,queue:1,stack:1,vector:1,map:1,set:1,bitset:1,multiset:1,multimap:1,unordered_set:1,unordered_map:1,unordered_multiset:1,unordered_multimap:1,array:1,shared_ptr:1}};return{dM:{k:a,i:"</",c:[hljs.CLCM,hljs.CBLCLM,hljs.QSM,{cN:"string",b:"'\\\\?.",e:"'",i:"."},{cN:"number",b:"\\b(\\d+(\\.\\d*)?|\\.\\d+)(u|U|l|L|ul|UL|f|F)"},hljs.CNM,{cN:"preprocessor",b:"#",e:"$"},{cN:"stl_container",b:"\\b(deque|list|queue|stack|vector|map|set|bitset|multiset|multimap|unordered_map|unordered_set|unordered_multiset|unordered_multimap|array)\\s*<",e:">",k:a,r:10,c:["self"]}]}}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
hljs.initHighlightingOnLoad();
</script>



<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<h1>Machine learning to predict good weight lifting form using sensor data</h1>

<h2>Heather Geiger: December 14,2017</h2>

<h3>Pre-processing data</h3>

<p>Read in data.</p>

<p>Data was already downloaded from here:</p>

<p><a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv</a></p>

<p><a href="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv">https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv</a></p>

<p>Source of this data:</p>

<p>Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human &#39;13) . Stuttgart, Germany: ACM SIGCHI, 2013</p>

<p><a href="http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har">http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har</a></p>

<pre><code class="r">training &lt;- read.csv(&quot;pml-training.csv&quot;,header=TRUE,stringsAsFactors=FALSE)
testing &lt;- read.csv(&quot;pml-testing.csv&quot;,header=TRUE,stringsAsFactors=FALSE)
</code></pre>

<p>Let&#39;s explore the data a bit.</p>

<p>First, how many rows and columns are there in each set?</p>

<pre><code class="r">dim(training)
</code></pre>

<pre><code>## [1] 19622   160
</code></pre>

<pre><code class="r">dim(testing)
</code></pre>

<pre><code>## [1]  20 160
</code></pre>

<p>A lot of columns!
But maybe some of these are not needed.</p>

<p>Going to look through the testing data to get a better sense of it.
This will be quite verbose, so not displaying this part.</p>

<pre><code class="r">testing[,1:20]
testing[,21:40]
testing[,41:60]
testing[,61:80]
testing[,81:100]
testing[,101:120]
testing[,121:140]
testing[,141:160]
</code></pre>

<p>Looks like there are quite a few columns that are all NA&#39;s in the test data.</p>

<p>Let&#39;s remove these from both the training and test sets.</p>

<p>Also, the first column is just equal to the row number, so let&#39;s remove that as well.</p>

<pre><code class="r">columns_to_remove &lt;- c(1,as.numeric(as.vector(which(apply(testing,2,function(x)length(which(is.na(x) == FALSE))) == 0))))

training &lt;- training[,setdiff(1:ncol(training),columns_to_remove)]
testing &lt;- testing[,setdiff(1:ncol(testing),columns_to_remove)]
</code></pre>

<p>Are the people in the training and test data sets the same?</p>

<pre><code class="r">table(training$user_name)
</code></pre>

<pre><code>## 
##   adelmo carlitos  charles   eurico   jeremy    pedro 
##     3892     3112     3536     3070     3402     2610
</code></pre>

<pre><code class="r">table(testing$user_name)
</code></pre>

<pre><code>## 
##   adelmo carlitos  charles   eurico   jeremy    pedro 
##        1        3        1        4        8        3
</code></pre>

<p>Yes. Good to know.</p>

<p>When we looked at all columns above, we saw some other columns we might want to remove.</p>

<p>For example, new_window is always &ldquo;no&rdquo; in the test data.</p>

<p>Timestamp variables can probably be removed as well.</p>

<p>Finally, the name of num_window implies it may be related to the windows of time convention, but we should look at this in more detail.</p>

<p>How does this variable compare per &ldquo;classe&rdquo; variable in the training set?</p>

<pre><code class="r">length(unique(training[,&quot;num_window&quot;]))
</code></pre>

<pre><code>## [1] 858
</code></pre>

<pre><code class="r">unique_num_window_per_classe &lt;- aggregate(num_window ~ classe,data=training,FUN=function(x)length(unique(x)))
unique_num_window_per_classe 
</code></pre>

<pre><code>##   classe num_window
## 1      A        242
## 2      B        168
## 3      C        151
## 4      D        141
## 5      E        156
</code></pre>

<pre><code class="r">sum(unique_num_window_per_classe[,2])
</code></pre>

<pre><code>## [1] 858
</code></pre>

<p>The values for num_window appear to be mutually exclusive for each value in classe.</p>

<p>This suggests it may be some sort of marker for the window rather than giving actual information about the physical lifts.</p>

<p>So, let&#39;s remove this variable as well.</p>

<p>Also remove problem_id column from testing. This is the last column.</p>

<pre><code class="r">additional_columns_to_remove &lt;- match(c(&quot;raw_timestamp_part_1&quot;,&quot;raw_timestamp_part_2&quot;,&quot;cvtd_timestamp&quot;,&quot;new_window&quot;,&quot;num_window&quot;),colnames(training))

training &lt;- training[,setdiff(1:ncol(training),additional_columns_to_remove)]
testing &lt;- testing[,setdiff(1:ncol(testing),additional_columns_to_remove)]

testing &lt;- testing[,1:(ncol(testing) - 1)]
</code></pre>

<p>Final pre-processing step is just to convert variables to factors as needed.</p>

<p>For each column, check the number of possible values for that column in training.</p>

<p>If very few, examine more and consider converting to factor.</p>

<p>Look at all but the first and last column, since we know we need to convert user_name and classe to factor.</p>

<pre><code class="r">num_unique_values_per_column &lt;- c()

for(i in 2:(ncol(training) - 1))
{
num_unique_values_per_column &lt;- c(num_unique_values_per_column,length(unique(training[,i])))
}

head(num_unique_values_per_column[order(num_unique_values_per_column)])
</code></pre>

<pre><code>## [1]  29  43  66  69  70 140
</code></pre>

<p>Looks like all the other variables are definitely numeric. So, just convert user_name and classe to factor.</p>

<pre><code class="r">training$user_name &lt;- factor(training$user_name)
training$classe &lt;- factor(training$classe)
testing$user_name &lt;- factor(testing$user_name)
</code></pre>

<h3>Building a model using machine learning</h3>

<p>Now, let&#39;s do the actual machine learning part.</p>

<p>First, remove user name.</p>

<p>Though it might improve our results a bit, it could also result in a less generalizable model if we want to apply these results to other people.</p>

<pre><code class="r">training_no_user_name &lt;- training[,2:ncol(training)]
</code></pre>

<p>Now, let&#39;s try starting with a nice simple model, rpart.</p>

<p>We&#39;ll do a simple holdout method (70%/30% split) to get a general sense of accuracy.</p>

<p>Then we can run cross-validation if this looks promising.</p>

<pre><code class="r">library(caret)
</code></pre>

<pre><code>## Loading required package: lattice
</code></pre>

<pre><code>## Loading required package: ggplot2
</code></pre>

<pre><code class="r">set.seed(1392)

#Partition by a combination of user name and classe.
#Even though we are not including user name in model, we don&#39;t want any one user overrepresented.

training_indices &lt;- createDataPartition(y = paste0(training$user_name,&quot;-&quot;,training$classe),p=0.7,list = FALSE)
testing_indices &lt;- setdiff(1:nrow(training),training_indices)

training_for_machine_learning &lt;- training_no_user_name[training_indices,]
testing_for_machine_learning &lt;- training_no_user_name[testing_indices,]

rpart_model &lt;- train(classe ~ .,data=training_for_machine_learning,method=&quot;rpart&quot;)
rpart_predictions &lt;- predict(rpart_model,newdata=training_for_machine_learning[,1:(ncol(training_for_machine_learning) - 1)])
confusionMatrix(data=rpart_predictions,reference = training_for_machine_learning$classe)
</code></pre>

<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 3561 1124 1117 1010  354
##          B   52  896   78  399  345
##          C  284  641 1204  846  673
##          D    0    0    0    0    0
##          E   11    0    0    0 1156
## 
## Overall Statistics
##                                           
##                Accuracy : 0.4957          
##                  95% CI : (0.4874, 0.5041)
##     No Information Rate : 0.2842          
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.3409          
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9112  0.33672  0.50188    0.000  0.45728
## Specificity            0.6337  0.92119  0.78471    1.000  0.99902
## Pos Pred Value         0.4969  0.50621  0.33004      NaN  0.99057
## Neg Pred Value         0.9473  0.85268  0.88172    0.836  0.89097
## Prevalence             0.2842  0.19351  0.17446    0.164  0.18384
## Detection Rate         0.2590  0.06516  0.08756    0.000  0.08407
## Detection Prevalence   0.5211  0.12872  0.26529    0.000  0.08487
## Balanced Accuracy      0.7725  0.62895  0.64329    0.500  0.72815
</code></pre>

<p>Accuracy is not good at all. Looks like we will need to use a more complex model, even if takes more time.</p>

<p>Let&#39;s use the random forest model.</p>

<p>Default parameters should be fine here. </p>

<p>Defaults are 500 trees, which seems reasonable, along with mtry = sqrt of variable number ~ 7 which also seems reasonable.</p>

<p>We will run 10-fold cross-validation, so no need to use the holdout method.</p>

<p>We can use all rows in training, just leaving out 1/10 at a time.</p>

<pre><code class="r">library(randomForest)
</code></pre>

<pre><code>## randomForest 4.6-12
</code></pre>

<pre><code>## Type rfNews() to see new features/changes/bug fixes.
</code></pre>

<pre><code>## 
## Attaching package: &#39;randomForest&#39;
</code></pre>

<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin
</code></pre>

<pre><code class="r">permutation_training_rows &lt;- sample(1:nrow(training),replace=FALSE)

starts_kfolds &lt;- 1962*1:10 - 1961
ends_kfolds &lt;- 1962*1:10;ends_kfolds[10] &lt;- 19622

accuracy_per_kfold &lt;- c()

for(i in 1:10)
{
this_k_range &lt;- permutation_training_rows[starts_kfolds[i]:ends_kfolds[i]]
not_this_k &lt;- setdiff(1:nrow(training),this_k_range)
rf_model &lt;- randomForest(classe ~ .,data=training_no_user_name[not_this_k,],keep.forest=TRUE, proximity=FALSE,importance=TRUE,do.trace=100)
rf_predictions &lt;- predict(rf_model,newdata=training_no_user_name[this_k_range,1:(ncol(training_no_user_name) - 1)])
accuracy_per_kfold &lt;- c(accuracy_per_kfold,confusionMatrix(data=rf_predictions,reference = training_no_user_name$classe[this_k_range])$overall[1])
}
</code></pre>

<pre><code>## ntree      OOB      1      2      3      4      5
##   100:   0.39%  0.12%  0.44%  0.65%  0.86%  0.09%
##   200:   0.37%  0.12%  0.47%  0.49%  0.76%  0.18%
##   300:   0.37%  0.12%  0.50%  0.52%  0.73%  0.15%
##   400:   0.36%  0.10%  0.44%  0.49%  0.80%  0.15%
##   500:   0.35%  0.10%  0.44%  0.49%  0.76%  0.15%
## ntree      OOB      1      2      3      4      5
##   100:   0.41%  0.04%  0.56%  0.52%  0.97%  0.22%
##   200:   0.35%  0.06%  0.44%  0.39%  0.83%  0.22%
##   300:   0.33%  0.04%  0.41%  0.39%  0.80%  0.22%
##   400:   0.32%  0.06%  0.44%  0.39%  0.69%  0.22%
##   500:   0.32%  0.04%  0.41%  0.39%  0.76%  0.22%
## ntree      OOB      1      2      3      4      5
##   100:   0.42%  0.08%  0.44%  0.72%  0.93%  0.21%
##   200:   0.40%  0.06%  0.50%  0.69%  0.75%  0.21%
##   300:   0.38%  0.06%  0.50%  0.59%  0.75%  0.21%
##   400:   0.35%  0.06%  0.47%  0.56%  0.72%  0.15%
##   500:   0.35%  0.08%  0.44%  0.56%  0.72%  0.15%
## ntree      OOB      1      2      3      4      5
##   100:   0.42%  0.04%  0.61%  0.59%  0.82%  0.31%
##   200:   0.35%  0.02%  0.50%  0.46%  0.72%  0.25%
##   300:   0.35%  0.00%  0.56%  0.49%  0.69%  0.25%
##   400:   0.33%  0.02%  0.47%  0.46%  0.69%  0.25%
##   500:   0.31%  0.00%  0.35%  0.46%  0.72%  0.22%
## ntree      OOB      1      2      3      4      5
##   100:   0.42%  0.06%  0.61%  0.55%  0.83%  0.28%
##   200:   0.36%  0.06%  0.50%  0.42%  0.80%  0.22%
##   300:   0.33%  0.04%  0.50%  0.45%  0.70%  0.19%
##   400:   0.36%  0.04%  0.44%  0.55%  0.73%  0.25%
##   500:   0.33%  0.02%  0.44%  0.39%  0.76%  0.25%
## ntree      OOB      1      2      3      4      5
##   100:   0.41%  0.06%  0.56%  0.46%  1.00%  0.22%
##   200:   0.37%  0.04%  0.53%  0.40%  0.93%  0.19%
##   300:   0.35%  0.02%  0.50%  0.40%  0.89%  0.15%
##   400:   0.32%  0.04%  0.47%  0.36%  0.79%  0.12%
##   500:   0.32%  0.04%  0.44%  0.36%  0.79%  0.19%
## ntree      OOB      1      2      3      4      5
##   100:   0.40%  0.06%  0.50%  0.55%  0.94%  0.22%
##   200:   0.36%  0.04%  0.44%  0.52%  0.80%  0.22%
##   300:   0.32%  0.02%  0.38%  0.45%  0.80%  0.15%
##   400:   0.32%  0.02%  0.44%  0.39%  0.76%  0.19%
##   500:   0.32%  0.02%  0.44%  0.42%  0.80%  0.12%
## ntree      OOB      1      2      3      4      5
##   100:   0.42%  0.04%  0.58%  0.61%  0.83%  0.31%
##   200:   0.39%  0.04%  0.53%  0.55%  0.69%  0.34%
##   300:   0.35%  0.04%  0.41%  0.61%  0.66%  0.25%
##   400:   0.34%  0.04%  0.41%  0.52%  0.69%  0.25%
##   500:   0.33%  0.04%  0.41%  0.52%  0.73%  0.19%
## ntree      OOB      1      2      3      4      5
##   100:   0.39%  0.12%  0.47%  0.65%  0.70%  0.18%
##   200:   0.37%  0.06%  0.50%  0.55%  0.76%  0.18%
##   300:   0.35%  0.06%  0.47%  0.52%  0.73%  0.18%
##   400:   0.36%  0.06%  0.41%  0.55%  0.76%  0.22%
##   500:   0.32%  0.06%  0.38%  0.49%  0.63%  0.22%
## ntree      OOB      1      2      3      4      5
##   100:   0.44%  0.10%  0.53%  0.68%  0.87%  0.25%
##   200:   0.36%  0.08%  0.38%  0.55%  0.83%  0.18%
##   300:   0.39%  0.08%  0.53%  0.49%  0.87%  0.18%
##   400:   0.36%  0.06%  0.47%  0.49%  0.83%  0.15%
##   500:   0.37%  0.06%  0.50%  0.52%  0.83%  0.18%
</code></pre>

<pre><code class="r">accuracy_per_kfold
</code></pre>

<pre><code>##  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy  Accuracy 
## 0.9943935 0.9969419 0.9974516 0.9959225 0.9959225 0.9989806 0.9959225 
##  Accuracy  Accuracy  Accuracy 
## 0.9979613 0.9969419 0.9984725
</code></pre>

<p>Accuracy seems quite good!</p>

<p>Let&#39;s make a model including all rows in training now.</p>

<pre><code class="r">rf_model &lt;- randomForest(classe ~ .,data=training_no_user_name,keep.forest=TRUE, proximity=FALSE,importance=TRUE,do.trace=100)
</code></pre>

<pre><code>## ntree      OOB      1      2      3      4      5
##   100:   0.35%  0.05%  0.40%  0.56%  0.78%  0.17%
##   200:   0.29%  0.05%  0.32%  0.38%  0.78%  0.11%
##   300:   0.27%  0.04%  0.26%  0.38%  0.72%  0.14%
##   400:   0.28%  0.04%  0.29%  0.47%  0.65%  0.11%
##   500:   0.28%  0.04%  0.32%  0.44%  0.62%  0.14%
</code></pre>

<p>Finally, predict for the 20 testing cases we started with, where classe is unknown.</p>

<pre><code class="r">testing_predictions &lt;- predict(rf_model,newdata=testing)
</code></pre>

<h3>Conclusions</h3>

<p>The accuracy on the 10 iterations of our k-fold cross validation was quite good (99.4-99.9%). In theory our out-of-sample error should be relatively similar to the 0.1-0.6% we see with the cross validation, which would be quite good.</p>

<p>Based on these accuracy rates, there is a pretty good chance that we will get all 20 instances right in the testing set.</p>

<p>It would be interesting to test this model on a set based on users besides the 6 included in this initial study.</p>

<p>Even though we excluded user name in the model, other users might have substantially different movement patterns than these 6 individuals, which could reduce the predictive power of the model and increase our out-of-sample error.</p>

</body>

</html>
